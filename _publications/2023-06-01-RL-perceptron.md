---
layout: publication
when: Jun 2023
title: The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions
authors: Nishil Patel, Sebastian Lee, <u>Stefano Sarao Mannelli</u>, Sebastian Goldt, Adrew Saxe
abstract: Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Experiments on variants of the Procgen game "Bossfight" and Arcade Learning Environment game "Pong" also show such a speed-accuracy trade-off in practice. Together, these results take a step towards closing the gap between theory and practice in high-dimensional RL.
thumbnail-img:
cover-img:
article_link: https://arxiv.org/abs/2306.10404
journal:
abstract_length_home: 100
tags: [reinforcement learning, curriculum learning]
---
